{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fda3492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "try:\n",
    "    import PyPDF2\n",
    "except ModuleNotFoundError:\n",
    "    !pip install PyPDF2\n",
    "    \n",
    "try:\n",
    "    import openai\n",
    "except ModuleNotFoundError:\n",
    "    !pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9b335b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write paper link   \n",
    "url=\"https://arxiv.org/pdf/1706.03762.pdf\"\n",
    "\n",
    "#Give minimum words for your summary\n",
    "minimum_words=300\n",
    "\n",
    "#Choose the preferred language\n",
    "lang=\"English\"\n",
    "\n",
    "#Give a name to your summary (optional)\n",
    "article_name=\"paper.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fd543dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s -o $article_name $url\n",
    "# Set the string that will contain the summary     \n",
    "pdf_summary_text = \"\"\n",
    "# Read the PDF file using PyPDF2\n",
    "pdf_file = open(article_name, 'rb')\n",
    "pdf_reader = PyPDF2.PdfReader(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "586da2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(demand):\n",
    "    # Call the OpenAI API to generate a response to the 'demand' parameter using a GPT-3.5 model called \"gpt-3.5-turbo\".\n",
    "    response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        # Set the context by sending a system message to the model informing the user that they are a helpful research assistant.\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful research assistant.\"},\n",
    "        # Send the 'demand' parameter as a user message to the model.\n",
    "        {\"role\": \"user\", \"content\": demand},\n",
    "        ],\n",
    "    )\n",
    "    # Return the generated response as a string. The content of the first message choice returned by the API response.\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b5b47ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing page 0\n",
      "Summarizing page 1\n",
      "Summarizing page 2\n",
      "Summarizing page 3\n",
      "Summarizing page 4\n",
      "Summarizing page 5\n",
      "Summarizing page 6\n",
      "Summarizing page 7\n",
      "Summarizing page 8\n",
      "Summarizing page 9\n",
      "Summarizing page 10\n",
      "Summarizing page 11\n",
      "Summarizing page 12\n",
      "Summarizing page 13\n",
      "Summarizing page 14\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "page_sums=[]\n",
    "\n",
    "# Loop through all the pages in the PDF file\n",
    "for page_num in range(len(pdf_reader.pages)):\n",
    "    # Extract the text from the page\n",
    "    page_text = pdf_reader.pages[page_num].extract_text().lower()\n",
    "\n",
    "    max_retries = 10\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            demand = f\"Summarize this in {lang} : {page_text}\"\n",
    "            page_summary = ask(demand)\n",
    "            page_sums.append(page_summary)\n",
    "            print(\"Summarizing page \" + str(page_num))\n",
    "            break  # If the API call is successful, break the loop\n",
    "        except openai.errors.RateLimitError as e:\n",
    "            print(f\"Caught RateLimitError: {e}\")\n",
    "            retries += 1\n",
    "            if retries < max_retries:\n",
    "                print(f\"Waiting for 10 seconds before retrying. Retry attempt {retries}/{max_retries}.\")\n",
    "                time.sleep(10)\n",
    "            else:\n",
    "                print(f\"Reached maximum retries ({max_retries}). Exiting.\")\n",
    "                break\n",
    "    else:\n",
    "        # If the loop has finished and no successful API call was made, skip this iteration\n",
    "        continue\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e88441e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper discusses a new approach to sequence transduction models in machine translation that eliminates the need for complex neural networks with encoders and decoders. Instead, the authors propose a simple network architecture called the Transformer, which solely relies on attention mechanisms, dispensing with recurrence and convolutions entirely. The results of experiments on two machine translation tasks demonstrate that the Transformer models perform better in quality, are more parallelizable, and require significantly less time to train. The authors also present evidence that the Transformer generalizes well to other tasks beyond machine translation.\n",
      "The article discusses the challenges with traditional recurrent language models in terms of parallelization and memory constraints. The proposed solution is the transformer model architecture, which relies on an attention mechanism to draw global dependencies between input and output, rather than using sequence-aligned recurrent neural networks or convolution. The model has been shown to achieve state-of-the-art results in translation quality and is fully auto-regressive. The paper provides a detailed description of the model's architecture, including stacked self-attention and point-wise, fully connected layers for both the encoder and decoder.\n",
      "This section describes the architecture of the transformer model which has two main components: the encoder and the decoder. The encoder is made up of six identical layers, each having two sub-layers: a self-attention mechanism and a fully connected feed-forward network. The decoder, also made up of six layers, adds a third sub-layer to perform multi-head attention over the output of the encoder. Residual connections and layer normalization are used in both the encoder and decoder, with output dimensions of 512. The attention function maps a query and a set of key-value pairs to an output, where the output is a weighted sum of the values based on the compatibility of the query with the corresponding key.\n",
      "This passage discusses two types of attention mechanisms in neural networks: scaled dot-product attention and multi-head attention. Scaled dot-product attention involves computing the dot products of queries and keys, dividing them by the square root of the dimension of the keys, and applying a softmax function to obtain weights on the values. Multi-head attention involves linearly projecting the queries, keys, and values multiple times and performing the attention function in parallel on each projection. The resulting output values are concatenated and projected again to produce the final values. The passage also mentions that dot-product attention can have performance issues with large values of dk, so the dot products are scaled to counteract this effect.\n",
      "The use of multi-head attention in the Transformer model allows for jointly attending to information from different representation subspaces at different positions. In this work, 8 parallel attention layers or heads are employed, with each head having reduced dimensions of 64. The Transformer uses multi-head attention in three different ways: encoder-decoder attention, encoder self-attention, and decoder self-attention. Additionally, each layer in the encoder and decoder contains a fully connected feed-forward network. The model uses learned embeddings and a softmax function to convert decoder output to predicted next-token probabilities. Finally, positional encoding is used to inject information about the relative or absolute position of the sequence.\n",
      "This passage discusses the comparison of different layer types used in sequence transduction tasks, such as mapping one variable-length sequence of symbol representations to another sequence of the same length. The focus is on self-attention layers and their benefits over recurrent and convolutional layers. Self-attention has a constant number of sequentially executed operations, making it faster than recurrent layers for smaller sequence lengths. The passage also describes the use of positional encodings to help the model learn to attend by relative positions. The maximum path length between any two input and output positions in different layer types is also compared. The use of self-attention layers could be restricted to a neighborhood of size r to improve computational performance for tasks involving very long sequences.\n",
      "The article discusses an approach to increase the maximum path length in a convolutional network by centering the input sequence around the output position, which requires a stack of convolutional layers. They propose using self-attention to decrease the complexity and yield more interpretable models. The models are trained on the WMT 2014 English-German dataset and split tokens into a vocabulary of about 37000 tokens. They use the Adam optimizer with dropout regularization during training. The big models are trained for 300,000 steps for 3.5 days on one machine with eight Nvidia P100 GPUs.\n",
      "The transformer model performs better than previous state-of-the-art models in English-to-German and English-to-French translation tasks with a lower training cost. The base Transformer model outperforms all previously published models and ensembles, while the Big Transformer model achieves a new state-of-the-art BLEU score of 28.4 on the English-to-German task and a BLEU score of 41.0 on the English-to-French task. Different variations of the Transformer model were also tested, with the number of attention heads and dimensions affecting the quality of translation. Label smoothing was also employed during training to improve accuracy and BLEU score. The results are presented in Table 2 and Table 3.\n",
      "Table 3 shows variations on the transformer architecture for English-to-German translation. Values not listed are the same as the base model. Perplexities are shown per wordpiece according to byte-pair encoding, and not comparable to per-word perplexities. The table shows that smaller attention key size reduces model quality, larger models perform better, and dropout is helpful to avoid overfitting. In row (e), replacing sinusoidal positional encoding with learned positional embeddings shows nearly identical results to the base model. Table 4 shows that the transformer model generalizes well to English constituency parsing. Different models were evaluated, and the transformer model performed comparably or better than previous models on this task.\n",
      "The article discusses the limitations of recurrent neural network sequence-to-sequence models and introduces the transformer model, which is based entirely on attention. The authors trained a 4-layer transformer on the Wall Street Journal portion of the Penn Treebank and achieved state-of-the-art results on translation tasks. They are excited about the future applications of attention-based models and plan to extend the transformer to handle other input and output modalities. The code used to train and evaluate their models is available on GitHub.\n",
      "This text lists various research papers on different techniques and methodologies used in machine learning and natural language processing, including empirical evaluation of gated recurrent neural networks on sequence modeling, recurrent neural network grammars, convolutional sequence to sequence learning, deep residual learning for image recognition, self-training PCFG grammars with latent annotations, exploring the limits of language modeling, can active memory replace attention?, neural GPUs learn algorithms, neural machine translation in linear time, structured attention networks, factorization tricks for LSTM networks, a structured self-attentive sentence embedding, multi-task sequence to sequence learning, effective approaches to attention-based neural machine translation, building a large annotated corpus of English, and effective self-training for parsing.\n",
      "This is a list of academic papers on natural language processing, including topics such as attention models, summarization, tree annotation, language models, neural machine translation, dropout for preventing overfitting, end-to-end memory networks, sequence to sequence learning, computer vision architecture, and fast and accurate parsing. The papers were published between 2006 and 2017.\n",
      "This passage discusses how American governments have passed laws since 2009 that make the registration or voting process more difficult. Figure 3 shows an example of the attention mechanism used to identify long-distance dependencies in the encoding process. Many of the attention heads attend to the distant dependency of the verb \"making\" to complete the phrase \"making...more difficult.\"\n",
      "This is a collection of text fragments, including a statement about how the law may never be perfect, but should be applied fairly. The last fragment includes a figure showing two attention heads, possibly related to resolving anaphora. Attention for the word \"its\" appears very precise.\n",
      "The text discusses the imperfections of the law, but emphasizes that the fair application of the law is crucial. The author believes that this is currently lacking. This is followed by several examples from a study on attention heads in a neural network model, which demonstrate the different tasks performed by the heads at layer 5 of 6.\n"
     ]
    }
   ],
   "source": [
    "pdf_summary_text='\\n'.join(page_sums)\n",
    "page_to_page_summary = article_name.replace(os.path.splitext(article_name)[1], \".ptp_summary.txt\")\n",
    "\n",
    "# Create the page summaries\n",
    "with open(page_to_page_summary, \"w+\", encoding=\"utf-8\") as file:\n",
    "    file.write(pdf_summary_text)\n",
    "print(pdf_summary_text)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4043c7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper introduces a new neural network architecture called the Transformer, which eliminates the need for complex encoders and decoders and instead solely relies on attention mechanisms to draw global dependencies between input and output. The Transformer model achieves state-of-the-art results in machine translation quality while being fully auto-regressive and more parallelizable, requiring significantly less time to train. The paper provides a detailed description of the model's architecture, including stacked self-attention and point-wise fully connected layers for both the encoder and decoder. The attention function maps a query and a set of key-value pairs to an output, where the output is a weighted sum of the values based on the compatibility of the query with the corresponding key. Two types of attention mechanisms are employed: scaled dot-product attention and multi-head attention, with the latter allowing for jointly attending to information from different representation subspaces at different positions. The Transformer uses multi-head attention in three different ways: encoder-decoder attention, encoder self-attention, and decoder self-attention. Learned embeddings and a softmax function are used to convert decoder output to predicted next-token probabilities, while positional encoding is used to inject information about the absolute and relative positions of the tokens. The authors present evidence that the Transformer generalizes well to other tasks beyond machine translation. Overall, the Transformer architecture offers a promising alternative to traditional neural network architectures in sequence transduction models, with potential applications across various natural language processing tasks.\n",
      "The paper proposes a novel approach to sequence transduction models in machine translation using a simple network architecture called the Transformer. Unlike traditional models with encoders and decoders, this approach relies solely on attention mechanisms, eliminating the need for recurrence and convolutions, resulting in a more parallelizable model that requires less training time. The architecture consists of an encoder and decoder, each having six identical layers with sub-layers consisting of self-attention and fully connected feed-forward networks. Multi-head attention is used in three different ways to jointly attend to information from different representation subspaces at different positions, resulting in better translation quality. The use of learned embeddings and a softmax function converts decoder output to predicted next-token probabilities, with positional encoding used to provide information about the sequence order. The Transformer model not only performs well in translation tasks but also generalizes well to other tasks. Furthermore, the paper discusses the challenges with traditional neural network models, such as parallelization and memory constraints, and demonstrates how the Transformer model overcomes those challenges. Overall, the Transformer model has shown to be a significant advancement in machine translation and beyond.\n",
      "The paper proposes a new approach to sequence transduction models in machine translation that relies solely on attention mechanisms rather than complex neural networks with encoders and decoders. This approach is known as the Transformer model and eliminates the issues of parallelization and memory constraints associated with traditional recurrent language models. The model architecture includes stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. Multi-head attention and a fully connected feed-forward network are used in each layer of the encoder and decoder. The Transformer model achieves state-of-the-art results in translation quality, is fully auto-regressive, and generalizes well to other tasks beyond machine translation. The paper provides a detailed description of the model's architecture and details two types of attention mechanisms: scaled dot-product attention and multi-head attention. The use of multi-head attention in the Transformer model allows for jointly attending to information from different representation subspaces at different positions. The model also uses learned embeddings and a softmax function to convert decoder output to predicted next-token probabilities and is supplemented with positional encoding.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ultimate_summary = \"\"\n",
    "page_limit = 5\n",
    "division=int(len(page_sums) / page_limit)\n",
    "\n",
    "if len(page_sums) > page_limit:\n",
    "    # divide the list into smaller parts\n",
    "    smaller_lists = [page_sums[i:i + page_limit] for i in range(0, len(page_sums), page_limit)]\n",
    "\n",
    "    # concatenate the items in each smaller list\n",
    "    concat_list = [''.join(smaller_list) for smaller_list in smaller_lists]\n",
    "\n",
    "    for i in concat_list:\n",
    "        first_summary = ' '.join(pdf_summary_text.split(' ')[:500])\n",
    "        \n",
    "        # Generate a summary for each smaller list and concatenate them\n",
    "        demand = f\"Create summary of this text in {lang} consisting maximum {minimum_words} words. {first_summary}\"\n",
    "        ultimate_summary_part = ask(demand)\n",
    "        ultimate_summary += ultimate_summary_part + \"\\n\"\n",
    "    \n",
    "    # Generate a final summary from the concatenated summaries    \n",
    "    demand = f\"Create summary of this text in {lang} from these page summaries. Calculate tokens left and use maximum tokens you can.   {ultimate_summary}\"\n",
    "    ultimate_summary_part = ask(demand)\n",
    "    ultimate_summary_file = article_name.replace(os.path.splitext(article_name)[1], \".summary.txt\")\n",
    "\n",
    "else:\n",
    "\n",
    "    first_summary = ' '.join(pdf_summary_text.split(' ')[:500])\n",
    "    demand = f\"Create a summary of this text in {lang}. The text must have minimum {minimum_words} words. Calculate tokens left and use maximum tokens you can. {first_summary}\"\n",
    "    ultimate_summary = ask(demand)\n",
    "    ultimate_summary_file = article_name.replace(os.path.splitext(article_name)[1], \".summary.txt\")\n",
    "    \n",
    "with open(ultimate_summary_file, \"w+\", encoding=\"utf-8\") as file2:\n",
    "    file2.write(ultimate_summary)\n",
    "\n",
    "print(ultimate_summary)\n",
    "file2.close()\n",
    "pdf_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeca88a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
